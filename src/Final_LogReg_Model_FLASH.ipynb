{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-02T12:34:43.547866Z",
     "start_time": "2025-06-02T12:08:38.780513Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GroupKFold, cross_validate\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "\n",
    "# Load dataset\n",
    "file_path = \"tdauditor_flashdeconv_with_instrument.tsv\"\n",
    "df = pd.read_csv(file_path, delimiter=\"\\t\")\n",
    "\n",
    "# Drop rows with missing target values\n",
    "df = df.dropna(subset=['hit'])\n",
    "\n",
    "# Handle missing Instrument values\n",
    "df['Instrument'] = df['Instrument'].fillna('Orbitrap Eclipse')\n",
    "\n",
    "# Define features and target (now including Instrument)\n",
    "features = df.drop(columns=['hit', 'SourceFile', 'NativeID'])\n",
    "X = features\n",
    "y = df[\"hit\"]\n",
    "\n",
    "# Split unique filenames into train/test/predict groups (to avoid data leakage)\n",
    "filenames = df[\"SourceFile\"].unique()\n",
    "train_filenames, temp_filenames = train_test_split(filenames, test_size=0.3, random_state=42)\n",
    "test_filenames, predict_filenames = train_test_split(temp_filenames, test_size=0.5, random_state=42)\n",
    "\n",
    "# Create corresponding dataframes\n",
    "train_df = df[df[\"SourceFile\"].isin(train_filenames)]\n",
    "test_df = df[df[\"SourceFile\"].isin(test_filenames)]\n",
    "predict_df_flash = df[df[\"SourceFile\"].isin(predict_filenames)]\n",
    "\n",
    "# Separate features and targets\n",
    "X_train, y_train = train_df[features.columns], train_df[\"hit\"]\n",
    "X_test, y_test = test_df[features.columns], test_df[\"hit\"]\n",
    "X_predict, y_predict_true = predict_df_flash[features.columns], predict_df_flash[\"hit\"]\n",
    "\n",
    "# Replace the loop with:\n",
    "X_train = X_train.copy()\n",
    "X_test = X_test.copy()\n",
    "X_predict = X_predict.copy()\n",
    "\n",
    "for dataset in [X_train, X_test, X_predict]:\n",
    "    dataset.loc[:, 'MatchedToDeconvolution'] = dataset['MatchedToDeconvolution'].astype(int)\n",
    "    \n",
    "# Enhanced one-hot encoding function\n",
    "def onehot_encode(df):\n",
    "    df = pd.get_dummies(df, columns=['mzMLDissociation'], prefix='dissoc')\n",
    "    df = pd.get_dummies(df, columns=['Instrument'], prefix='inst')\n",
    "    return df\n",
    "\n",
    "X_train = onehot_encode(X_train)\n",
    "X_test = onehot_encode(X_test)\n",
    "X_predict = onehot_encode(X_predict)\n",
    "\n",
    "# Align columns across all datasets\n",
    "train_cols = X_train.columns\n",
    "X_test = X_test.reindex(columns=train_cols, fill_value=0)\n",
    "X_predict = X_predict.reindex(columns=train_cols, fill_value=0)\n",
    "\n",
    "# Feature Engineering\n",
    "def add_features(df):\n",
    "    dissoc_cols = [col for col in df.columns if col.startswith('dissoc_')]\n",
    "    for col in dissoc_cols:\n",
    "        df[f'{col}_x_LongestTag'] = df[col] * df['LongestTag']\n",
    "    \n",
    "    inst_cols = [col for col in df.columns if col.startswith('inst_')]\n",
    "    for col in inst_cols:\n",
    "        df[f'{col}_x_DeconvPeakCount'] = df[col] * df['DeconvPeakCount']\n",
    "    \n",
    "    df['IsLongTagReliable'] = ((df['LongestTag'] < 25) & (df['DeconvPeakCount'] < 250))\n",
    "    return df\n",
    "\n",
    "X_train = add_features(X_train)\n",
    "X_test = add_features(X_test)\n",
    "X_predict = add_features(X_predict)\n",
    "\n",
    "# Fill missing values\n",
    "X_train = X_train.fillna(X_train.mean())\n",
    "X_test = X_test.fillna(X_train.mean())\n",
    "X_predict = X_predict.fillna(X_train.mean())\n",
    "\n",
    "# Create pipeline\n",
    "pipeline = ImbPipeline([\n",
    "    ('smote', SMOTE(random_state=42)),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('model', LogisticRegression(C=0.01, random_state=42, \n",
    "                                max_iter=1000, class_weight='balanced'))\n",
    "])\n",
    "\n",
    "# Cross-validation\n",
    "groups = train_df['SourceFile']\n",
    "cv = GroupKFold(n_splits=5)\n",
    "scoring = ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']\n",
    "\n",
    "cv_results = cross_validate(\n",
    "    pipeline,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    groups=groups,\n",
    "    cv=cv,\n",
    "    scoring=scoring,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Print CV results\n",
    "print(\"\\nCross-Validation Results:\")\n",
    "for metric in scoring:\n",
    "    mean_score = np.mean(cv_results[f'test_{metric}'])\n",
    "    std_score = np.std(cv_results[f'test_{metric}'])\n",
    "    print(f\"{metric}: {mean_score:.3f} Â± {std_score:.3f}\")\n",
    "\n",
    "# Train final model\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Get feature names and coefficients\n",
    "feature_mapping = {\n",
    "    **{f'dissoc_{name}': f'Dissoc_{name}' for name in df['mzMLDissociation'].unique()},\n",
    "    **{f'inst_{name}': f'Inst_{name}' for name in df['Instrument'].unique()}\n",
    "}\n",
    "\n",
    "feature_names = [feature_mapping.get(col, col) for col in X_train.columns]\n",
    "final_model = pipeline.named_steps['model']\n",
    "\n",
    "coef_df_flash = pd.DataFrame({\n",
    "    \"Feature\": feature_names,\n",
    "    \"Coefficient\": final_model.coef_[0]\n",
    "}).sort_values(\"Coefficient\", key=lambda x: abs(x), ascending=False)\n",
    "\n",
    "# Display top features\n",
    "print(\"\\nTop 20 Most Influential Features:\")\n",
    "print(coef_df_flash.head(20)[[\"Feature\", \"Coefficient\"]])\n",
    "\n",
    "# Evaluate on test set using pipeline\n",
    "X_test_processed = pipeline[:-1].transform(X_test)  # Apply SMOTE and scaling\n",
    "y_test_pred = pipeline.named_steps['model'].predict(X_test_processed)\n",
    "\n",
    "print(f\"\\nTest Accuracy: {accuracy_score(y_test, y_test_pred):.4f}\")\n",
    "print(\"Classification Report (Test Set):\\n\", classification_report(y_test, y_test_pred))\n",
    "\n",
    "# Prediction set results\n",
    "X_predict_processed = pipeline[:-1].transform(X_predict)\n",
    "y_pred_proba = pipeline.named_steps['model'].predict_proba(X_predict_processed)[:, 1]\n",
    "y_pred_adjusted = (y_pred_proba >= 0.5).astype(int)\n",
    "\n",
    "print(\"\\nPrediction Set Results (first 20 predictions):\")\n",
    "print(\"Predicted Probabilities:\", y_pred_proba[:20])\n",
    "print(\"Predicted Labels:\", y_pred_adjusted[:20])\n",
    "print(\"True Labels:\", y_predict_true.values[:20])\n",
    "\n",
    "# Save predictions\n",
    "predict_df_flash['Predicted_Probability'] = y_pred_proba\n",
    "predict_df_flash['Predicted_Hit'] = y_pred_adjusted"
   ],
   "execution_count": 1,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T12:41:35.729518Z",
     "start_time": "2025-06-03T12:41:35.032406Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "# Set consistent style\n",
    "sns.set(style=\"whitegrid\")  # or try 'darkgrid', 'ticks', etc.\n",
    "\n",
    "\n",
    "# Get predicted probabilities for class 1 (hit)\n",
    "y_proba = predict_df_flash['Predicted_Probability'].values\n",
    "y_true = y_predict_true.values\n",
    "\n",
    "# Compute calibration curve\n",
    "prob_true, prob_pred = calibration_curve(y_true, y_proba, n_bins=10)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.plot(prob_pred, prob_true, marker='o', label='Model Calibration')\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', color='gray', label='Perfect Calibration')\n",
    "plt.xlabel('Predicted Probability')\n",
    "plt.ylabel('True Probability')\n",
    "plt.title('Calibration Curve (Prediction Set)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "d596ef987aae80b3",
   "execution_count": 6,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T12:44:30.422109Z",
     "start_time": "2025-06-03T12:44:29.916041Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Create DataFrames for each category\n",
    "instrument_features = coef_df_flash[coef_df_flash['Feature'].str.startswith('Inst_')]\n",
    "dissociation_features = coef_df_flash[coef_df_flash['Feature'].str.startswith('Dissoc_')]\n",
    "other_features = coef_df_flash[~coef_df_flash['Feature'].str.startswith(('Inst_', 'Dissoc_'))]\n",
    "\n",
    "# Set style\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.figure(figsize=(15, 12))\n",
    "\n",
    "# 1. Instrument Weights Plot\n",
    "plt.subplot(3, 1, 1)\n",
    "sns.barplot(data=instrument_features, x='Coefficient', y='Feature', palette='viridis')\n",
    "plt.title('Instrument Feature Weights', fontsize=14)\n",
    "plt.xlabel('Coefficient Value', fontsize=12)\n",
    "plt.ylabel('Instrument Type', fontsize=12)\n",
    "\n",
    "# 2. Dissociation Weights Plot\n",
    "plt.subplot(3, 1, 2)\n",
    "sns.barplot(data=dissociation_features, x='Coefficient', y='Feature', palette='magma')\n",
    "plt.title('Dissociation Method Weights', fontsize=14)\n",
    "plt.xlabel('Coefficient Value', fontsize=12)\n",
    "plt.ylabel('Dissociation Type', fontsize=12)\n",
    "\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print interpretation\n",
    "print(\"\\nKey Insights:\")\n",
    "print(\"1. Instrument Effects:\")\n",
    "for idx, row in instrument_features.iterrows():\n",
    "    print(f\"   - {row['Feature']}: {'Increases' if row['Coefficient'] > 0 else 'Decreases'} probability of hit (strength: {abs(row['Coefficient']):.2f})\")\n",
    "\n",
    "print(\"\\n2. Dissociation Effects:\")\n",
    "for idx, row in dissociation_features.iterrows():\n",
    "    print(f\"   - {row['Feature']}: {'Increases' if row['Coefficient'] > 0 else 'Decreases'} probability of hit (strength: {abs(row['Coefficient']):.2f})\")\n",
    "\n",
    "print(\"\\n3. Other Major Factors:\")\n",
    "for idx, row in other_features.head(10).iterrows():\n",
    "    effect = \"positively\" if row['Coefficient'] > 0 else \"negatively\"\n",
    "    print(f\"   - {row['Feature']}: Affects hits {effect} (strength: {abs(row['Coefficient']):.2f})\")\n",
    "    if 'x_' in row['Feature']:\n",
    "        parts = row['Feature'].split('_x_')\n",
    "        print(f\"     (Interaction between {parts[0]} and {parts[1]})\")"
   ],
   "id": "6ae44105493601ae",
   "execution_count": 7,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T12:52:41.663017Z",
     "start_time": "2025-06-03T12:52:40.751351Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib_venn import venn2\n",
    "\n",
    "# Filter to just the scans from the selected file\n",
    "file_name = \"20200214_rmi049_75umPLRPS_Allen_CD3_F4_b\"\n",
    "subset_df = predict_df_flash[predict_df_flash[\"SourceFile\"] == file_name]\n",
    "\n",
    "# Create sets of scan numbers\n",
    "logit_set = set(subset_df[subset_df[\"Predicted_Probability\"] > 0.5][\"ScanNumber\"])\n",
    "hit_set = set(subset_df[subset_df[\"hit\"] == 1][\"ScanNumber\"])\n",
    "\n",
    "#print(logit_set)\n",
    "#print(hit_set)\n",
    "#print(subset_df.shape)\n",
    "#print(subset_df[['SourceFile', 'ScanNumber', 'Predicted_Probability', 'hit']].head())\n",
    "\n",
    "# Plot the Venn diagram\n",
    "plt.figure(figsize=(6, 6))\n",
    "venn2([logit_set, hit_set], set_labels=('Logit > 0.5', 'Identified (Hit Report)'))\n",
    "plt.title(f\"Scan Overlap for File:\\n{file_name}\")\n",
    "plt.show()"
   ],
   "id": "b06dd4b3202bb05a",
   "execution_count": 8,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T13:01:01.661917Z",
     "start_time": "2025-06-03T13:01:01.646964Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "print((predict_df_flash['Predicted_Hit'] == 1).sum())\n",
    "print((predict_df_flash['hit'] == 1).sum())"
   ],
   "id": "f84ca869a8e4c54d",
   "execution_count": 10,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": "",
   "id": "2af49a5dbb08683b",
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
